{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Requirement already satisfied: torch in d:\\simon\\anaconda\\lib\\site-packages (1.11.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for joblib: [Errno 2] No such file or directory: 'd:\\\\simon\\\\anaconda\\\\lib\\\\site-packages\\\\joblib-1.1.0.dist-info\\\\METADATA'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: torchvision in d:\\simon\\anaconda\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: torchaudio in d:\\simon\\anaconda\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: typing_extensions in d:\\simon\\anaconda\\lib\\site-packages (from torch) (3.10.0.2)\n",
      "Requirement already satisfied: numpy in d:\\simon\\anaconda\\lib\\site-packages (from torchvision) (1.20.3)\n",
      "Requirement already satisfied: requests in d:\\simon\\anaconda\\lib\\site-packages (from torchvision) (2.26.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\simon\\anaconda\\lib\\site-packages (from torchvision) (8.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\simon\\anaconda\\lib\\site-packages (from requests->torchvision) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\simon\\anaconda\\lib\\site-packages (from requests->torchvision) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\simon\\anaconda\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\simon\\anaconda\\lib\\site-packages (from requests->torchvision) (2021.10.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\n",
      "ERROR: No matching distribution found for pickle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting varname\n",
      "  Using cached varname-0.11.0-py3-none-any.whl (23 kB)\n",
      "Collecting executing<2.0,>=1.2\n",
      "  Using cached executing-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Installing collected packages: executing, varname\n",
      "Successfully installed executing-1.2.0 varname-0.11.0\n",
      "['d:\\\\mycode\\\\BSc\\\\python', 'd:\\\\simon\\\\Anaconda\\\\python39.zip', 'd:\\\\simon\\\\Anaconda\\\\DLLs', 'd:\\\\simon\\\\Anaconda\\\\lib', 'd:\\\\simon\\\\Anaconda', '', 'd:\\\\simon\\\\Anaconda\\\\lib\\\\site-packages', 'd:\\\\simon\\\\Anaconda\\\\lib\\\\site-packages\\\\locket-0.2.1-py3.9.egg', 'd:\\\\simon\\\\Anaconda\\\\lib\\\\site-packages\\\\win32', 'd:\\\\simon\\\\Anaconda\\\\lib\\\\site-packages\\\\win32\\\\lib', 'd:\\\\simon\\\\Anaconda\\\\lib\\\\site-packages\\\\Pythonwin', 'd:\\\\simon\\\\Anaconda\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\simon\\\\.ipython', 'C:\\\\Python311\\\\Lib\\\\site-packages', 'C:\\\\Python311\\\\Lib\\\\site-packages']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -cikit-learn (d:\\simon\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (d:\\simon\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (d:\\simon\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (d:\\simon\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (d:\\simon\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (d:\\simon\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (d:\\simon\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (d:\\simon\\anaconda\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U scikit-learn\n",
    "!pip3 install torch torchvision torchaudio\n",
    "!pip3 install pickle\n",
    "!pip3 install -U varname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import math \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "import pickle "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unpacking the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/sampled_filters_train.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data):\n",
    "    #in data\n",
    "    #wavelength (4000000, 149)\n",
    "    #spectra (4000000, 149)\n",
    "    #X (4000000, 37)\n",
    "    #y (4000000, 4)\n",
    "    #z (4000000, 1)\n",
    "    #zmin (float)\n",
    "    #zmax (float)\n",
    "    #filter_names (string list), len = 37\n",
    "    df_X = pd.DataFrame(data['X'])\n",
    "    outlier_columns = df_X.columns[df_X.gt(58.8).any(axis = 0)]\n",
    "    df_X = df_X.drop(outlier_columns, axis = 1)\n",
    "    X = df_X.to_numpy()\n",
    "    y = pd.DataFrame(data['y'])\n",
    "    z = pd.DataFrame(data['z'])\n",
    "    wavelengths = data['wavelengths']\n",
    "    spectra = data['spectra']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_normalized = scaler.fit_transform(df_X)\n",
    "\n",
    "    names=data['filter_names']\n",
    "    filter_wls = [int(name[1:-1].rstrip('W')) for name in names]\n",
    "    filter_wls = [element for i, element in enumerate(filter_wls) if i not in outlier_columns]\n",
    "    indices=[0,10,30,500,15000,800000]\n",
    "    return filter_wls, wavelengths, spectra, X, X_normalized, y, z\n",
    "filter_wls, wavelengths, spectra, X, X_normalized, y, z = get_data(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        return x\n",
    "\n",
    "class SingleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(SingleConv, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Down, self).__init__()\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Up, self).__init__()\n",
    "        self.up = nn.ConvTranspose1d(in_channels, in_channels//2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diff = x2.size()[2] - x1.size()[2]\n",
    "        x1 = F.pad(x1, (diff // 2, (diff + 1) // 2))\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class BigUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BigUNet, self).__init__()\n",
    "        self.conv = DoubleConv(1, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.up2 = Up(512, 256)\n",
    "        self.up3 = Up(256, 128)\n",
    "        self.up4 = Up(128, 64)\n",
    "        self.convOut = nn.Conv1d(64, 1, kernel_size = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x3 = self.up2(x4, x3)\n",
    "        x2 = self.up3(x3, x2)\n",
    "        x1 = self.up4(x2, x1)\n",
    "        x = self.convOut(x1)\n",
    "        return x\n",
    "    \n",
    "class MedBigUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MedBigUNet, self).__init__()\n",
    "        self.conv = DoubleConv(1, 32)\n",
    "        self.down1 = Down(32, 64)\n",
    "        self.down2 = Down(64, 128)\n",
    "        self.down3 = Down(128, 256)\n",
    "        self.up2 = Up(256, 128)\n",
    "        self.up3 = Up(128, 64)\n",
    "        self.up4 = Up(64, 32)\n",
    "        self.convOut = nn.Conv1d(32, 1, kernel_size = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x3 = self.up2(x4, x3)\n",
    "        x2 = self.up3(x3, x2)\n",
    "        x1 = self.up4(x2, x1)\n",
    "        x = self.convOut(x1)\n",
    "        return x\n",
    "\n",
    "class MediumUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MediumUNet, self).__init__()\n",
    "        self.conv = DoubleConv(1, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.up1 = Up(256, 128)\n",
    "        self.up2 = Up(128, 64)\n",
    "        self.convOut = nn.Conv1d(64, 1, kernel_size = 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.conv(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x2 = self.up1(x3, x2)\n",
    "        x1 = self.up2(x2, x1)\n",
    "        x = self.convOut(x1)\n",
    "        return x\n",
    "\n",
    "class SmallUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallUNet, self).__init__()\n",
    "        self.conv = DoubleConv(1, 32)\n",
    "        self.down1 = DoubleConv(32, 64)\n",
    "        self.down2 = DoubleConv(64, 128)\n",
    "        self.up1 = Up(128, 64)\n",
    "        self.up2 = Up(64, 32)\n",
    "        self.convOut = nn.Conv1d(32, 1, kernel_size = 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.conv(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x2 = self.up1(x3, x2)\n",
    "        x1 = self.up2(x2, x1)\n",
    "        x = self.convOut(x1)\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_betaAlpha_schedule(T = 300, s=0.005):\n",
    "    x = torch.linspace(0, T, T+1)\n",
    "    alphas_cumprod = torch.cos(((x / T) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    betas = torch.clip(betas, 0.0001, 0.9999)\n",
    "    alpha_bar = np.cumprod(np.array([1-betas[i] for i in range(len(betas))]))\n",
    "    return betas, alpha_bar\n",
    "\n",
    "def add_noise(X, t, beta_t, alpha_bar):\n",
    "    if t == 0:\n",
    "        X_tm1 = X.numpy()\n",
    "    else:\n",
    "        mu, sigma = math.sqrt(alpha_bar[t - 1]) * X, (1 - alpha_bar[t - 1]) * np.identity(len(X))\n",
    "        X_tm1 = np.random.multivariate_normal(mu, sigma)\n",
    "    mu, sigma = math.sqrt(1 - beta_t[t]) * X_tm1, (beta_t[t]) * np.identity(len(X_tm1))\n",
    "    X_t = np.random.multivariate_normal(mu, sigma)\n",
    "    pred_noise =  X_t - X_tm1\n",
    "    return X_t, pred_noise\n",
    "\n",
    "def gen_dataset(data, T = 300):\n",
    "\n",
    "    X_t_arr = []\n",
    "    X_noise_arr = []\n",
    "\n",
    "    beta_t, alpha_bar = cos_betaAlpha_schedule(T = T)\n",
    "\n",
    "    for d in data:\n",
    "        t = np.random.randint(0, T)\n",
    "        X_t, X_noise = add_noise(d, t, beta_t, alpha_bar)\n",
    "        X_t_arr.append(X_t)\n",
    "        X_noise_arr.append(X_noise)\n",
    "\n",
    "    return X_t_arr, X_noise_arr\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dict):\n",
    "        self.data_dict = data_dict\n",
    "        self.keys = list(data_dict.keys())\n",
    "        self.length = len(data_dict[self.keys[0]])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = {key: self.data_dict[key][index] for key in self.keys}\n",
    "        return data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, n = len(X_normalized), batch_size = 256, EPOCHS = 50):\n",
    "    #loss\n",
    "    loss_fn = nn.MSELoss()\n",
    "    #optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "    #loss data\n",
    "    running_loss_arr = []\n",
    "\n",
    "    #training\n",
    "    model.train(True)\n",
    "    with open(\"foo.txt\", \"w\") as f:\n",
    "        f.write(\"start \\n\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        with open(\"foo.txt\", \"a\") as f:\n",
    "            f.write(\"epoch: \" + str(epoch) + \"\\n\")\n",
    "        # generate data_loader\n",
    "        train_dataloader = DataLoader(MyDataset({\"X\": X_normalized[:n]}), batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        \n",
    "        print('EPOCH ', epoch, \":\")\n",
    "        \n",
    "        running_loss = 0\n",
    "        last_loss = 0\n",
    "\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            # Every data instance is an input + label pair\n",
    "            X_t, X_tm1 = gen_dataset(data['X'])\n",
    "            X_t = torch.tensor(X_t).unsqueeze(1).double().to(device)\n",
    "            X_tm1 = torch.tensor(X_tm1).unsqueeze(1).double().to(device)\n",
    "            # Zero gradients for every batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Make predictions for this batch\n",
    "            outputs = model(X_t)\n",
    "            \n",
    "            # Compute the loss and its gradients\n",
    "            loss = loss_fn(outputs, X_tm1)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # Adjust learning weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Gather data\n",
    "            running_loss += loss.item()\n",
    "            if (i+1) % 100 == 0:\n",
    "                last_loss = running_loss / 100 # loss per batch\n",
    "                tb_x = epoch * len(train_dataloader) + i + 1\n",
    "                running_loss_arr.append([tb_x, last_loss])\n",
    "                running_loss = 0.\n",
    "                with open(\"foo.txt\", \"a\") as f:\n",
    "                    f.write(str(last_loss) + \"\\n\")\n",
    "        scheduler.step()\n",
    "\n",
    "        print('LOSS train: ', running_loss_arr[-1])\n",
    "\n",
    "    model.train(False)\n",
    "    return np.array(running_loss_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_big = BigUNet().double().to(device)\n",
    "model_medBig = MedBigUNet().double().to(device)\n",
    "model_medium = MediumUNet().double().to(device)\n",
    "model_small = SmallUNet().double().to(device)\n",
    "modelList = [model_big, model_medBig, model_medium, model_small]\n",
    "modelNames = [\"model_big\", \"model_medBig\", \"model_medium\", \"model_small\"]\n",
    "#modelList = [model_big]\n",
    "#modelNames = [\"model_big\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################\n",
      "n:  512000\n",
      "batch_size:  64\n",
      "Epochs:  1\n",
      "model:  BigUNet(\n",
      "  (conv): DoubleConv(\n",
      "    (conv1): Conv1d(1, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu1): ReLU(inplace=True)\n",
      "    (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (down1): Down(\n",
      "    (conv): DoubleConv(\n",
      "      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (down2): Down(\n",
      "    (conv): DoubleConv(\n",
      "      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (down3): Down(\n",
      "    (conv): DoubleConv(\n",
      "      (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (up2): Up(\n",
      "    (up): ConvTranspose1d(512, 256, kernel_size=(2,), stride=(2,))\n",
      "    (conv): DoubleConv(\n",
      "      (conv1): Conv1d(512, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (up3): Up(\n",
      "    (up): ConvTranspose1d(256, 128, kernel_size=(2,), stride=(2,))\n",
      "    (conv): DoubleConv(\n",
      "      (conv1): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (up4): Up(\n",
      "    (up): ConvTranspose1d(128, 64, kernel_size=(2,), stride=(2,))\n",
      "    (conv): DoubleConv(\n",
      "      (conv1): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (convOut): Conv1d(64, 1, kernel_size=(1,), stride=(1,))\n",
      ")\n",
      "\n",
      "EPOCH  0 :\n",
      "LOSS train:  [8000, 0.03727647640845384]\n",
      "#####################################\n",
      "#####################################\n",
      "n:  512000\n",
      "batch_size:  64\n",
      "Epochs:  1\n",
      "model:  MedBigUNet(\n",
      "  (conv): DoubleConv(\n",
      "    (conv1): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu1): ReLU(inplace=True)\n",
      "    (conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (down1): Down(\n",
      "    (conv): DoubleConv(\n",
      "      (conv1): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (down2): Down(\n",
      "    (conv): DoubleConv(\n",
      "      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (down3): Down(\n",
      "    (conv): DoubleConv(\n",
      "      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (up2): Up(\n",
      "    (up): ConvTranspose1d(256, 128, kernel_size=(2,), stride=(2,))\n",
      "    (conv): DoubleConv(\n",
      "      (conv1): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (up3): Up(\n",
      "    (up): ConvTranspose1d(128, 64, kernel_size=(2,), stride=(2,))\n",
      "    (conv): DoubleConv(\n",
      "      (conv1): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (up4): Up(\n",
      "    (up): ConvTranspose1d(64, 32, kernel_size=(2,), stride=(2,))\n",
      "    (conv): DoubleConv(\n",
      "      (conv1): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (convOut): Conv1d(32, 1, kernel_size=(1,), stride=(1,))\n",
      ")\n",
      "\n",
      "EPOCH  0 :\n",
      "LOSS train:  [8000, 0.039832338050618844]\n",
      "#####################################\n",
      "#####################################\n",
      "n:  512000\n",
      "batch_size:  64\n",
      "Epochs:  1\n",
      "model:  MediumUNet(\n",
      "  (conv): DoubleConv(\n",
      "    (conv1): Conv1d(1, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu1): ReLU(inplace=True)\n",
      "    (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (down1): Down(\n",
      "    (conv): DoubleConv(\n",
      "      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (down2): Down(\n",
      "    (conv): DoubleConv(\n",
      "      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (up1): Up(\n",
      "    (up): ConvTranspose1d(256, 128, kernel_size=(2,), stride=(2,))\n",
      "    (conv): DoubleConv(\n",
      "      (conv1): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (up2): Up(\n",
      "    (up): ConvTranspose1d(128, 64, kernel_size=(2,), stride=(2,))\n",
      "    (conv): DoubleConv(\n",
      "      (conv1): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (convOut): Conv1d(64, 1, kernel_size=(1,), stride=(1,))\n",
      ")\n",
      "\n",
      "EPOCH  0 :\n",
      "LOSS train:  [8000, 0.0367976759935559]\n",
      "#####################################\n",
      "#####################################\n",
      "n:  512000\n",
      "batch_size:  64\n",
      "Epochs:  1\n",
      "model:  SmallUNet(\n",
      "  (conv): DoubleConv(\n",
      "    (conv1): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu1): ReLU(inplace=True)\n",
      "    (conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (down1): DoubleConv(\n",
      "    (conv1): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu1): ReLU(inplace=True)\n",
      "    (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (down2): DoubleConv(\n",
      "    (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu1): ReLU(inplace=True)\n",
      "    (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (up1): Up(\n",
      "    (up): ConvTranspose1d(128, 64, kernel_size=(2,), stride=(2,))\n",
      "    (conv): DoubleConv(\n",
      "      (conv1): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (up2): Up(\n",
      "    (up): ConvTranspose1d(64, 32, kernel_size=(2,), stride=(2,))\n",
      "    (conv): DoubleConv(\n",
      "      (conv1): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (convOut): Conv1d(32, 1, kernel_size=(1,), stride=(1,))\n",
      ")\n",
      "\n",
      "EPOCH  0 :\n",
      "LOSS train:  [8000, 0.0392169140436574]\n",
      "#####################################\n"
     ]
    }
   ],
   "source": [
    "# train all models\n",
    "n = 512000\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "for name, model in zip(modelNames, modelList):\n",
    "    print(\"#####################################\")\n",
    "    print(\"n: \", n)\n",
    "    print(\"batch_size: \", batch_size)\n",
    "    print(\"Epochs: \", epochs)\n",
    "    print(\"model: \", model)\n",
    "    \n",
    "    running_loss_arr = train_model(model, n = n, batch_size = batch_size, EPOCHS = epochs)\n",
    "\n",
    "    torch.save(model.state_dict(), 'runs/model_param_' + name + '.pt')\n",
    "    df_loss = pd.DataFrame({\"time\": running_loss_arr.reshape(-1, 2).transpose()[0], \"loss\": running_loss_arr.reshape(-1, 2).transpose()[1]})\n",
    "    df_loss.to_csv('output/losses' + name + '.csv', index = False)\n",
    "    print(\"#####################################\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise(model, X, T = 300):\n",
    "    X = torch.tensor(X).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    for _ in range(T):\n",
    "        X_noise = model(X)\n",
    "        X = X - X_noise\n",
    "    return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7101ca937a6cefa303d920fd1335fe82956cc294edbf5f7bc268a5a56c54bb64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
